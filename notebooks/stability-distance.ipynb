{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#The-permutation-stability-problem\" data-toc-modified-id=\"The-permutation-stability-problem-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>The permutation stability problem</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Relationship-between-$\\sigma$-and-entropy\" data-toc-modified-id=\"Relationship-between-$\\sigma$-and-entropy-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Relationship between $\\sigma$ and entropy</a></span></li><li><span><a href=\"#Background:-Sinkhorn-method\" data-toc-modified-id=\"Background:-Sinkhorn-method-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Background: Sinkhorn method</a></span></li><li><span><a href=\"#Measuring-permutation-stability-with-$\\delta$\" data-toc-modified-id=\"Measuring-permutation-stability-with-$\\delta$-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>Measuring permutation stability with $\\delta$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Proof-ideas\" data-toc-modified-id=\"Proof-ideas-1.0.3.1\"><span class=\"toc-item-num\">1.0.3.1&nbsp;&nbsp;</span>Proof ideas</a></span></li><li><span><a href=\"#Questions\" data-toc-modified-id=\"Questions-1.0.3.2\"><span class=\"toc-item-num\">1.0.3.2&nbsp;&nbsp;</span>Questions</a></span></li><li><span><a href=\"#Hypotheses\" data-toc-modified-id=\"Hypotheses-1.0.3.3\"><span class=\"toc-item-num\">1.0.3.3&nbsp;&nbsp;</span>Hypotheses</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The permutation stability problem\n",
    "\n",
    "Let $\\theta, \\theta'$ be source and target weight matrices in $\\mathbb{R}^{n \\times m}$, equivalently written $\\begin{bmatrix} w_1, w_2, \\dots, w_n \\end{bmatrix}$.\n",
    "Let the space of stochastic permutations (in matrix form) be $\\mathcal{T} = \\{P \\in \\mathbb{R}^{n \\times n} \\text{ such that } P\\mathbb{1} = \\mathbb{1} \\text{ and } \\mathbb{1}^\\top P  = \\mathbb{1}^\\top \\}$.\n",
    "Similarly, let the space of deterministic permutations be $\\mathcal{P} = \\{P \\in \\mathcal{T} \\text{ such that } P_{ij} \\in \\{0, 1\\} \\}$.\n",
    "\n",
    "<span style=\"color:red\">Here I am not sure what this $\\mathcal{T}$ corresponds to.</span> \n",
    "set of transport mappings, marginals (uniform marginals). \n",
    "\n",
    "Let $P^\\star = \\operatorname{argmin}_P \\langle \\theta, P [\\theta'] \\rangle_F$ be a permutation in $\\mathcal{P}$ minimizing the Frobenius distance $\\langle \\cdot, \\cdot \\rangle$ between matrices.\n",
    "\n",
    "We want to know the stability of $P$ to noise. In particular,\n",
    "> Given independent $Z_\\sigma, Z'_\\sigma \\in \\mathbb{R}^{n \\times m}$ randomly sampled from an isotropic Gaussian with mean 0 and standard deviation $\\sigma \\mathbb{I}$, what is\n",
    "\n",
    "$$P_\\sigma := \\mathbb{E}_Z \\operatorname{argmin}_P \\langle \\theta + Z_\\sigma, P [\\theta' + Z'_\\sigma] \\rangle_F$$\n",
    "\n",
    "Here we have $P_\\sigma \\in \\mathcal{T}$. We will assume permutations are in $\\mathcal{T}$ unless we explicitly say they are in $\\mathcal{P}$.\n",
    "\n",
    "<!-- -->\n",
    "*Note: WLOG suppose $P^\\star = \\mathbb{I}$ so that we can evaluate the stability of $P_Z$ as $1 - tr(P_Z) / n$.* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between $\\sigma$ and entropy\n",
    "\n",
    "<span style=\"color:red\">Shouldn't there be a $\\mathbf{-}$ sign $\\mathbb{E}[-\\log(P)]$ </span> \n",
    "\n",
    "<!-- <span style=\"color:red\"> Also is this really a valid formula? $p_{ij}\\in \\{0, 1\\}$, then $\\log p_{ij}$ is undefined? \n",
    "</span> -->\n",
    "\n",
    "Let $H(P) = \\mathbb{E}[\\log(P)] = \\sum_{i=1}^n \\sum_{j=1}^n (p_{ij} / n) \\log(p_{ij} / n)$ be the entropy of a permutation.\n",
    "*Note: we divide by $n$ so that $P$ is a joint distribution with uniformly distributed marginals $r$ and $c$, where $r(i) = c(j) = \\frac 1n$. This treats $P_\\sigma$ as a discrete transport plan.*\n",
    "\n",
    "Define the uniformly random permutation $P_{rand} := \\frac 1n \\mathbb{1} \\mathbb{1}^\\top$ (equivalent to having the transport plan be the product of the marginals).\n",
    "\n",
    "$H(P)$ is bounded by $\\log(n)$ and $2 \\log(n)$ when $P$ is equal to $P^\\star$ or $P_{rand}$ respectively.\n",
    "\n",
    "As $\\sigma \\to 0$, $P_\\sigma \\to P^\\star$ if there is a unique $P^\\star$, or to the mean of possible $P^\\star$ otherwise. The latter occurs if $\\theta'$ has vectors at positions $i$ and $j$ such that $w_i = w_j$ (such an equality defines a subspace along which all transpositions of $w_i$ and $w_j$ are mirrored). $H(P_\\sigma)$ is minimized as $\\sigma$ decreases.\n",
    "\n",
    "*Note: we can guarantee $P^\\star$ is unique by adding an $\\epsilon$ perturbation to any $\\theta$ or $\\theta'$ with equidistant weights. Or, we can lower bound $H(P_\\sigma)$ as $H(\\mathbb{E}[P^\\star])$, where the expectation is shorthand for averaging over the set of minimizing permutations.*\n",
    "\n",
    "As $\\sigma \\to \\infty$, $P_\\sigma \\to P_{rand}$, which is a uniformly random permutation. If for all $i, j$ we have $w_i = w_j$, then $P^\\star$ is also the uniform random permutation. $H(P_\\sigma)$ is maximized as $\\sigma$ increases.\n",
    "\n",
    "*Note: in the definition of $P_\\sigma$ we add IID noise to both $\\theta$ and $\\theta'$ in order to make the problem symmetric. Specifically, since $P^\\top_\\sigma = \\mathbb{E}_Z \\operatorname{argmin}_P \\langle \\theta' + Z_\\sigma, P [\\theta + Z'_\\sigma] \\rangle_F$, we have $H(P_\\sigma) = H(P^\\top_\\sigma)$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background: Sinkhorn method\n",
    "\n",
    "The Sinkhorn method (Cuturi 2013) finds minimizing permutations by applying an entropic regularization term.\n",
    "\n",
    "Let $C \\in \\mathbb{R}^{n \\times n}$ be a cost matrix where $C_{ij} = \\langle \\theta_i, \\theta'_j \\rangle_v$, where $\\langle \\cdot, \\cdot \\rangle_v$ is the Euclidean distance so that $\\langle PC \\rangle_F = \\langle \\theta, P[\\theta'] \\rangle_F$.\n",
    "\n",
    "Cuturi (2013) shows that for bounded entropy $H(P) < f(\\lambda)$, the permutation $P^\\lambda = \\operatorname{argmin}_P \\langle PC \\rangle_F -\\lambda H(P)$ can be solved by applying Sinkhorn's operator to uniquely rescale $\\exp(-C / \\lambda)$ to the transport plan $P^\\lambda$, where $\\exp$ is elementwise exponentiation. Taking $\\lambda \\to 0$ recovers the optimal permutation $P^\\star$.\n",
    "\n",
    "*Note: this works because the Lagrangian for $P^\\lambda$ has a solution of the form $\\alpha \\mathbb{I} \\exp(-C / \\lambda) \\mathbb{I} \\beta$ with positive scaling factors $\\alpha, \\beta \\in \\mathbb{R}^n$. Sinkhorn's theorem states every positive matrix can be rescaled to a particular doubly stochastic matrix (where every row or column sums to 1) by iteratively applying the Sinkhorn operator, which simply divides every row by its sum, and then every column by its sum. Thus, we can use the Sinkhorn operator to project $\\exp(-C / \\lambda)$ onto the solution $P^\\lambda$ in $\\mathcal{T}$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring permutation stability with $\\delta$\n",
    "\n",
    "To measure the stability of a permutation minimizing the distance between two matrices, we need a quantity $\\delta$ that describes the rate of change in $H(P_\\sigma)$ with respect to $\\sigma$, or equivalently, how quickly does $P_\\sigma$ go from $P^\\star$ to $P_{rand}$ as noise is introduced?\n",
    "\n",
    "Let $P^1(\\theta, \\theta') = Sink(\\exp(-C(\\theta, \\theta')))$ where $Sink$ is the Sinkhorn operator, $e$ is the elementwise exponential, and $C(\\theta, \\theta') = \\begin{bmatrix} \\langle \\theta_i, \\theta'_j \\rangle_v \\end{bmatrix}$. We also have $P^\\star(\\theta, \\theta') = \\mathbb{E}\\left[ \\operatorname{argmin}_P \\langle \\theta, P [\\theta'] \\rangle_F \\right]$ where the expectation is the average over the set of minimizing permutations. Then define:\n",
    "$$\\delta(\\theta, \\theta') := H(P^1(\\theta, \\theta')) - H(P^\\star(\\theta, \\theta'))$$\n",
    "\n",
    ">**Claim: $\\delta$ increases monotonically with $\\sigma$ and is approximately equal to $H(P_\\sigma)$** for all $\\theta, \\theta'$ perturbed by noise with fixed $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof ideas\n",
    "\n",
    "From a physics standpoint, $C$ is energy, $\\lambda$ is inverse temperature, and $\\frac{\\partial H(P^\\lambda)}{\\partial C} = C \\lambda$. (?)\n",
    "\n",
    "$H(P)$ is convex, does not depend on $C$, and is always minimized at $P_{rand}$. Since the Lagrangian $\\langle PC \\rangle_F -\\lambda H(P)$ adds a linear term, its minimum lies on a path $P^\\lambda$ between $P_{rand}$ and $P^\\star$. How quickly $\\lambda$ progresses on this path depends on $\\|\\frac{\\partial PC}{\\partial P}\\| = \\|C\\|$. *Note: while $H(P)$ is bounded because $P$ must be a distribution, $\\frac{\\partial H(P)}{\\partial P}$ is unbounded, thus $P^\\star$ is reached only when $\\lambda \\to 0$*\n",
    "\n",
    "---\n",
    "\n",
    "Ideally, we need to prove the following:\n",
    "> Proposition: for every $\\sigma$, there exists a $\\lambda = \\mathcal{O}(\\sigma)$ such that $P_\\sigma \\approx P^\\lambda$.\n",
    "\n",
    "Fix the cost matrix $C$ and let the cost of any permutation $P$ be $c(P) = \\langle PC \\rangle_F$. Let $C_\\sigma$ be the cost when some noise of magnitude $\\sigma$ is applied, and define $c_\\sigma(P) = \\langle PC_\\sigma \\rangle_F$. The entropy of $P_\\sigma$ can be rephrased as the problem: given $P$ and $P'$ where $c(P) < c(P')$, what is the probability that $c_\\sigma(P) > c_\\sigma(P')$?\n",
    "\n",
    "Let $\\mu = c(P) - c(P')$. When Gaussian noise is added to $\\theta$, the cost matrix is Gaussian (derived in another notebook), and $\\mathbb{P}[c_\\sigma(P) > c_\\sigma(P')] = \\mathbb{P}[\\mathcal{N}(\\mu, \\nu \\sigma) > 0]$ where $\\nu$ depends on the magnitude of the vectors in $\\theta$ and $\\theta'$ (not derived here).\n",
    "\n",
    "We can define a distribution over $\\mathcal{P}$ as follows. Let $Y_P(x) := \\mathbb{P}[c_\\sigma(P) < x]$. The probability that $P \\in \\mathcal{P}$ has minimum cost is given by $y(P) := \\int_{-\\infty}^\\infty Y_P(x) \\prod_{P' \\in \\mathcal{P}} (1 - \\bar{Y}_{P'}(x)) \\text{d}x$.\n",
    "\n",
    "**Claim: $y(P)$ for $P \\in \\mathcal{P}$ is a probability distribution on $\\mathcal{P}$, and $P_\\sigma = \\sum_{P \\in \\mathcal{P}} P y(P)$.**\n",
    "\n",
    "To get to entropy, note that each individual $y(P)$ indicates a Bernoulli distribution.\n",
    "\n",
    "**Claim: taking the sum of entropies of Bernoulli RVs with probability $y(P)$ over $P \\in \\mathcal{P}$ gives $H(P_\\sigma)$.**\n",
    "\n",
    "Problem: how to relate this back to $P^\\lambda$?\n",
    "\n",
    "*Perhaps a mean field theory approach would help?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "For brevity let $\\delta(\\theta) := \\delta(\\theta, \\theta)$.\n",
    "\n",
    "> Question: can we make $\\delta$ into a metric?\n",
    "\n",
    "Identity: no, $\\delta(\\theta)$ is not necessarily 0, or even the minimum $H(\\mathbb{E}[P^\\star])$. But for $\\delta(\\theta, \\theta')$, can we subtract some combination of $\\delta(\\theta)$ and $\\delta(\\theta')$ to get 0?\n",
    "\n",
    "Symmetry: yes, $\\delta(\\theta, \\theta') = \\delta(\\theta', \\theta)$.\n",
    "\n",
    "Triangle inequality: unknown? But we really want this so that $\\delta(\\theta, \\theta') \\geq \\delta(\\theta)$ and $\\delta(\\theta, \\theta') \\geq \\delta(\\theta')$.\n",
    "\n",
    "> Question: if we bound the distance between $\\theta$ and $\\theta'$, how is $\\delta(\\theta, \\theta')$ bounded?\n",
    "\n",
    "This would tie nicely into Ekansh's ideas about linear convergence modulo permutation of SGD.\n",
    "\n",
    "> Corollary: if $\\theta$ interpolates along a path between $\\theta_0$ and $\\theta^\\star$, how is $\\delta(\\theta, \\theta^\\star)$ bounded? Does it matter if the path is linear, or does only the distance to $\\theta^\\star$ matter?\n",
    "\n",
    "\n",
    "#### Hypotheses\n",
    "\n",
    "Let $\\theta^\\star$ be a trained weight matrix.\n",
    "1. $\\delta(\\theta)$ correlates with permutation stability of $\\theta$: that is, two independent training runs of $\\theta$ will have less permutation (and barrier) after training proportional to $\\delta(\\theta, \\theta)$.\n",
    "2. $\\delta(\\theta, \\theta^\\star) - \\delta(\\theta^\\star)$ correlates with barrier after training. Furthermore, we can determine a threshold of $\\delta(\\theta, \\theta^\\star) - \\delta(\\theta^\\star)$ below which two networks are LMC after training.\n",
    "3. Over training time $t$, $\\delta(\\theta_t)$ reduces as $t$ increases.\n",
    "4. If $\\theta$ is trained on one task and $\\theta'$ on another, $\\delta(\\theta, \\theta')$ is greater for more dissimilar tasks. Similarly, when $\\theta$ is pre-trained on one task and $\\theta^\\star$ has been additionally fine-tuned on another task, $\\delta(\\theta, \\theta^\\star)$ is greater if the tasks are more dissimilar.\n",
    "\n",
    "TODO... design experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
